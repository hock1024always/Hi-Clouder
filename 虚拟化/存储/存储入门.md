# 存储入门

学习的原文来自于公司技术大佬（内部资料仅内网可访问）：[SharedBlock Bible Lite](http://confluence.zstack.io/display/~wei.wang/SharedBlock+Bible+Lite)

这个是一个架构师给的学习路线，值得一看：https://zhuanlan.zhihu.com/p/528770730

图解清晰讲透了DAS、SAN、NAS：https://zhuanlan.zhihu.com/p/614992586

RAID（独立磁盘冗余阵列，Redundant Array of Independent Disks）是一种通过将多个物理硬盘驱动器组合成一个或多个逻辑单元来提高性能和/或提供数据冗余的技术。RAID技术旨在预防数据丢失，并能提升数据读写速度，具体取决于所使用的RAID级别。

FC（Fibre Channel，光纤通道）交换机是专门设计用于在存储区域网络（SAN）中进行高速数据传输的设备。它们通过光纤通道协议来连接服务器与存储系统，提供高带宽、低延迟的数据访问性能，适用于需要高性能和可靠性的应用环境，比如大规模数据库、虚拟化平台和大型企业的存储需求。

SCSI 协议：SCSI（Small Computer System Interface）即小型计算机系统接口，是一种用于计算机和外部设备（如硬盘、磁带驱动器、光驱等）之间进行数据传输的标准协议。

高密度的未知信息，太多不懂，记录起来很吃力，应该在阅读时同步记录不懂的地方的，但是太难了。针对这个问题，我需要重新看一遍先前的文章，针对不懂的、想要了解的点重点搜索、问人，重新整理笔记。今天开了个头，把SharedBlock 的架构前章节做了快速阅读和理解尝试，每天进步一点点！

# DAS直连附加存储

DAS（Direct Attached Storage，直连附加存储），指讲磁盘直接插在磁盘直接插在服务器上，配合服务器的接口、电路和服务器的其他设备（例如CPU、内存）提供服务。但是存在可用性和拓展性问题——

- 服务器故障时，业务会中断，无法提供存储服务
- 一台服务器盘位有限，无法提供更大的存储空间
- 为了保障数据安全和可用，通常需要使用RAID实现数据冗余，这进一步加剧存储空间不足问题



# RAID独立磁盘冗余阵列

RAID0、1、3、5、6、10清晰讲解：https://blog.csdn.net/NSD1907/article/details/122553338

RAID（独立磁盘冗余阵列，Redundant Array of Independent Disks）是一种通过将多个物理硬盘驱动器组合成一个或多个逻辑单元来提高性能和/或提供数据冗余的技术。RAID技术旨在预防数据丢失，并能提升数据读写速度，具体取决于所使用的RAID级别。常见的RAID级别包括：

- **RAID 0**：数据条带化，提供更高的性能，但没有冗余。（数据分成N份直接读写）
- **RAID 1**：数据镜像，提供冗余备份，即使一块硬盘故障，数据仍可恢复。（一半磁盘用于备份）
- **RAID 5**：数据和校验信息分散存储，提供冗余和较好的读写性能，最多承受一块硬盘故障。（分散数据和校验信息，利用校验信息还原损坏数据）
- **RAID 6**：类似于RAID 5，但具有双重冗余，能承受两块硬盘故障。（双重校验保证数据冗余）
- **RAID10：**RAID0和RAID1组合，从构建流程上看，首先基于RAID1将磁盘分为两份做数据镜像，再基于RAID0并发写数据实现数据条带化。（建议配合图食用，图上为逻理解，理解为先分片再冗余一半）

![e46f7d29ccac64558f1e1def5c8e7965](D:\github\CloudNote\图片\存储\e46f7d29ccac64558f1e1def5c8e7965.png)



# SAN存储区域网络



## 什么是SAN？

上述DAS存在可用性和拓展性问题，虽然使用RAID可以保证数据安全和存储高可用，但是无法解决服务器宕机时业务中断的情况。SAN（Storage Area Network，存储区域网络）主要是为了解决传统直接附加存储（DAS，Direct Attached Storage）在扩展性、性能和管理上的局限性。

SAN（Storage Area Network，存储区域网络），它将**存储设备从服务器中分离开**，使用专门设备来存放磁盘，通过网络连接服务器，服务器读取、写入数据都通过网络，那么这个设备上的存储可以同时暴露给多个服务器，任意一个服务器读取、写入的是同样的存储，那么所有服务器的角色就可以对等、从而实现一定的高可用。

**SAN的核心作用：存储设备与服务器分离，存储资源扩展。注意SAN是集中式存储架构，不是文件系统，OCFS2才是文件系统，OCFS2是SAN的早期解决方案。**

SAN的四部分——节点、交换机、控制器、（扩展柜）

- 节点：服务器、计算节点，从存储设备读取数据，向存储设备写入数据
- 交换机：通常为FC交换机，提供网络服务
- 存储控制器：上文的专用设备一般指的是存储控制器，磁盘插在这里
- 扩展柜：制器一般盘位有限，因此后面可能还会接一个 Expansion tray 即扩展柜

![img](D:\github\CloudNote\图片\存储\SAN-stor-config.png)



其实这里并没有透彻地点出NAS是什么，以及具体的协议、实现，只是抛出了一个拓扑架构的概念。更多细节待到原文中挖掘。



## SAN带来的新问题

将存储设备和服务器分离开后，已经实现了服务器和存储的高可用（存储的高可用通过RAID实现），网络的高可用成为新的问题。通过增加冗余交换机（Switch）可以实现网络的高可用性。

然而，多个节点访问同一个存储时，会出现资源抢占和重复读写的问题。为了解决这些问题，需要引入一种资源调度技术。



## 早期解决方案——OCFS2

OCFS2（Oracle Cluster File System 2，甲骨文集群文件系统2）

OCFS2 给了我们一个很棒的方案，它可以实现一种“共享文件系统”，区别与我们常用的普通的文件系统（本地文件系统，例如 Linux 上的 Ext4、XFS，Mac 上 APFS、Windows 上的 NTFS）——这些本地文件系统在有多个设备同时挂载使用时由于无法协调相互间的写入时机、重要资源分配（例如元数据）而导致文件系统损坏，OCFS2 通过 O2CB 完成了节点的管理、心跳，通过使用 DLM 对 inode 资源加锁来避免脏写、脏读（例如在其他节点写入过程中也写入数据，导致结果无法预期）。

除此之外，在大部分角度看来，OCFS2 和一个本地文件系统几乎一样的，能够创建目录、文件，能够读取、删除文件，这样我们只要像使用一个特殊的本地文件系统——在 ZStack 种叫做 SMP，Shared Mount Point，共享挂载点来使用就好了。

由于它具有文件系统所具有的大部分特性，因此 Qcow2 文件（也就是我们使用的虚拟机根盘、云盘、镜像的格式）可以直接存放在上面，还是可以写时分配空间的。可以通过简单的操作得知文件的名字、大小、属性，可以额外用文本文件或者什么保存你想保存的一些什么信息，读取的话可以用 cat、vim、head 或者一切你熟悉的 Linux 命令，简直美好的世界！

但是（你肯定能猜到这里有一个但是），有好的一面就有好坏的一面，OCFS2 的设计目标是一个通用文件系统——通用文件系统带来了刚才我们说的那些好的特性、功能，也带来了一些负面影响，例如性能、维护难度和部署复杂性。

其实部署复杂吗？并不算非常复杂，只是 ZStack 之前没有做自动化部署；运维呢？分布式的系统真正运维起来没几个简单的，当然多了一层文件系统也许确实复杂一些。

真正最影响我们的，是性能问题，OCFS2 可以通过调整 Cluster 对其性能做一些优化，但是最终我们测下来有一个问题难以解决：

文件系统会影响到 IO 的分布，大的 Cluster Size 更是会造成一定的写放大效应，在小文件随机读写上造成明显的削弱，特别是分层存储或带 Cache 的存储上，很容易写跑出 Cache，导致性能测试数据不理想、IO 曲线不平滑。

如果节点数多，性能将会更差。

此外，是它的控制平面性能，刚才提到 OCFS2 是一个面向通用的文件系统，因此其丰富的功能造成很多限制——

1. 在默认 Cluster Size 4KB 下只能管理 16TB 的存储
2. 理论上最多 32 个节点，实际上一般不会这么多，超过 10 个可能性能就会下降很厉害了

最终，我们决定重新实现一个新的共享存储对接方案。

Cluster Size：簇大小或块大小，是磁盘读写的最小单位

（看到这里有相当多名词看不懂了，是不是需要补充文件系统、IO相关知识？上述理论已经看了四五个小时了，老看理论也不太行，如何动手实践一下？）哈哈终于把硬盘、文件系统的基本知识补充了一些。



# SharedBlock

为了克服 OCFS2 的局限性，引入了 Shared Block 解决方案。Shared Block 由以下三个组件共同构成：

1. LVM：负责存储设备的映射与管理。
2. SANLock：提供基于租约的资源访问管理。
3. LVMLockd：将 SANLock 的功能与 LVM 集成。

 https://blog.csdn.net/qq1773304209/article/details/102468163

[块设备是一种具有一定结构的随机存取设备，对这种设备的读写是按块进行的，使用缓冲区来存放暂时的数据，待条件成熟后，从缓存一次性写入设备或者从设备一次性读到缓冲区。块设备包括硬盘、磁盘、U盘和SD卡等。](https://zhuanlan.zhihu.com/p/507214979)  还可以了解一下字符设备、网络设备。



## 架构

![image2018-11-9_10-24-0](D:\github\CloudNote\图片\存储\image2018-11-9_10-24-0.png)

> HBA 是 “Host Bus Adapter” 的缩写，即主机总线适配器，是一种连接主机和存储设备的硬件接口卡，通常插在服务器或主机的扩展槽中。其主要功能是将主机的 I/O 请求转换为存储设备能够理解的指令，并在主机和存储设备之间传输数据，实现主机与存储系统之间的高速数据通信。
>
> MultiPath：多路径机制，识别这些磁盘是否实际上是同一个磁盘，将实际是同一个磁盘的的这些磁盘组成一个多路径设备
>
> SCSI（Small Computer System Interface）即小型计算机系统接口，是一种用于计算机和外部设备（如硬盘、磁带驱动器、光驱等）之间进行数据传输的标准协议。
>
> 分布式文件系统有三大难点——（CAP？）
>
> - 集群维护，包括心跳、仲裁、节点管理等
> - 一致性，包括空间划分、文件管理、属性管理等这些元数据的一致性和文件的加锁、防止脏读脏写等这些对数据的保护
> - 网络稳定性，对网络抖动、中断和分区

主线从下往上看，很好懂，见原文。

主线完，还需要协调竞争数据情况，来个锁，采取了SanLockd+LVMLockd

## LVM

PV、VG、LV、PE、LE

![image-20250426155027681](D:\github\CloudNote\图片\存储\image-20250426155027681.png)

## Sanlock+LVMLockd

Sanlock（shared storage lock manager，共享存储锁管理），LVMLockd使用sanlock来进行管理。

![image2018-11-9_11-19-51](D:\github\CloudNote\图片\存储\image2018-11-9_11-19-51.png)