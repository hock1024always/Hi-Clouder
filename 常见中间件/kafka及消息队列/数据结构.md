# 数据结构

## 数组查询的复杂度为O(1)

数组必须要内存中一块**连续**的空间，并且数组中必须存放**相同**的数据类型。利用这个特性，我们只要知道了数组下标，也就是数据在数组中的位置

## 队列和栈的区别

先进先出和先进后出

### 1. 用队列实现栈

**核心思想**：使用**一个队列**，通过循环移位来模拟栈的后进先出特性。

**操作步骤**：

- **Push（入栈）**：
  1. 将新元素放入队列。
  2. 将队列中**原有**的所有元素依次出队并重新入队。
  3. 这样，新元素就被移动到了队列的最前端，相当于栈顶。
- **Pop（出栈）**：直接从队列前端出队即可。
- **Top（查看栈顶）**：返回队列前端的元素。
- **Empty（判断空）**：检查队列是否为空。

**示例**（使用一个队列）：
`push(1)`： 队列 `[1]`
`push(2)`： 先入队2 -> `[1, 2]`，再将1出队并入队 -> `[2, 1]`
`push(3)`： 先入队3 -> `[2, 1, 3]`，再将2,1出队并入队 -> `[3, 2, 1]`
`pop()`： 出队3 -> `[2, 1]`

------

### 2. 用栈实现队列

**核心思想**：使用**两个栈**，一个专用于输入（`inStack`），一个专用于输出（`outStack`）。

**操作步骤**：

- **Push（入队）**：所有新元素直接压入 `inStack`。
- **Pop（出队）/Peek（查看队首）**：
  1. 如果 `outStack` 为空，则将 `inStack` 中的**所有**元素依次弹出并压入 `outStack`。
  2. 从 `outStack` 弹出栈顶元素（即为队首元素）。
- **Empty（判断空）**：同时检查 `inStack` 和 `outStack` 是否都为空。

**示例**（使用两个栈）：
`push(1)`, `push(2)`： `inStack = [1, 2]`, `outStack = []`
`peek()`： `outStack`为空，将`inStack`元素转移 -> `inStack = []`, `outStack = [2, 1]`。返回 `outStack` 栈顶 `1`。
`pop()`： 从 `outStack` 弹出 `1`。
`push(3)`： 压入 `inStack` -> `inStack = [3]`, `outStack = [2]`。

**总结**：

- **栈 -> 队列**：核心是**两个栈，通过转移元素来反转顺序**。
- **队列 -> 栈**：核心是**一个队列，通过循环轮转来定位栈顶**。

## 平衡二叉树结构

平衡二叉树平衡的特性：

- 左右两个子树的高度差（平衡因子）的绝对值不超过1
- 左右两个子树都是一棵平衡二叉树

非平衡二叉树(左)和平衡二叉树(右)如下图所示：

![img](./图片/1735743-20201108165847587-114546797.png)

通过平衡的特性，可以有效的减少二叉树的深度，从而提高了查询的效率。

## 红黑树说一下，跳表说一下？

红黑树（Red-Black Tree）是一种自平衡的二叉搜索树，它在插入和删除操作后能够通过旋转和重新着色来保持树的平衡。红黑树的特点如下：

1. 每个节点都有一个颜色，红色或黑色。
2. 根节点是黑色的。
3. 每个叶子节点（NIL节点）都是黑色的。
4. **如果一个节点是红色的，则它的两个子节点都是黑色的**。
5. 从根节点到叶子节点或空子节点的**每条路径上，黑色节点的数量是相同的**。

红黑树通过这些特性来保持树的平衡，确保最长路径不超过最短路径的两倍，从而保证了在最坏情况下的搜索、插入和删除操作的时间复杂度都为O(logN)。

![image-20240725233526166](./图片/image-20240725233526166.png)

跳表（Skip List）是一种基于链表的数据结构，它通过添加多层索引来加速搜索操作。

![image-20240725233537853](./图片/image-20240725233537853.png)

跳表的特点如下：

1. 跳表中的数据是有序的。
2. 跳表中的每个节点都包含一个指向下一层和右侧节点的指针。

跳表通过多层索引的方式来加速搜索操作。最底层是一个普通的有序链表，而上面的每一层都是前一层的子集，每个节点在上一层都有一个指针指向它在下一层的对应节点。这样，在搜索时可以通过跳过一些节点，直接进入目标区域，从而减少搜索的时间复杂度。

跳表的平均搜索、插入和删除操作的时间复杂度都为O(logN)，与红黑树相比，跳表的实现更加简单，但空间复杂度稍高。跳表常用于需要高效搜索和插入操作的场景，如数据库、缓存等。

### 你知道什么地方用了红黑树和跳表吗？

- epoll 用了红黑树来保存监听的 socket
- redis 用了跳表来实现 zset

### 跳表时间复杂度？

![image-20240725233553870](./图片/image-20240725233553870.png)

- 搜索操作的时间复杂度：O(log n)，其中n是跳表中元素的数量。这是因为跳表中使用多级索引，可以通过跳跃的方式快速定位到目标元素所在的位置，从而将搜索的时间复杂度降低到对数级别。
- 插入和删除操作的时间复杂度：O(log n)，其中n是跳表中元素的数量。与搜索操作类似，插入和删除操作也可以通过跳跃的方式快速定位到需要插入或删除的位置，并进行相应的操作。因此，插入和删除的时间复杂度也是对数级别的。

### 红黑树的数据结构介绍一下？

红黑树是一种自平衡的二叉查找树，

![img](./图片/1716015879195-fb9dc798-dfe8-4bf5-99d0-33c92a26bf8b.webp)

具有以下特点：

1. 每个节点要么是红色，要么是黑色。
2. 根节点是黑色。
3. 每个叶子节点（NIL节点）是黑色。
4. 如果一个节点是红色，则其子节点必须是黑色。
5. 从任一节点到其每个叶子节点的所有路径都包含相同数目的黑色节点。

红黑树的自平衡性质可以保证在进行插入、删除等操作后，树的高度保持在O(log n)内，从而保持了较高的查找、插入和删除效率。下面是红黑树插入节点的过程，这左旋右旋的操作，就是为了自平衡。

![img](./图片/1716015879219-fb418c32-57d7-461a-908e-cb4b5f1feff7.gif)

### 红黑树和AVL树相比查询性能好还是插入性能好一些？

| **操作**      | **AVL 树**    | **红黑树**    |
| ------------- | ------------- | ------------- |
| **查询**      | ⭐⭐⭐⭐⭐（更快） | ⭐⭐⭐⭐          |
| **插入/删除** | ⭐⭐⭐           | ⭐⭐⭐⭐⭐（更快） |
| **平衡开销**  | 高            | 低            |

1、查询性能的对比：

- **AVL 树**：AVL 树是严格的平衡二叉搜索树，它要求每个节点的左右子树的高度差（平衡因子）不超过 1。这种严格的平衡特性使得 AVL 树的高度始终保持在 O(log n)，其中 n)是树中节点的数量。在进行查询操作时，由于树的高度相对较低且较为均匀，所以查找任意节点的时间复杂度稳定为 O(log n)。这意味着在理想情况下，AVL 树的查询效率非常高，能快速定位到目标节点。
- **红黑树**：红黑树是一种弱平衡的二叉搜索树，它通过颜色标记和特定的规则（如每个节点要么是红色，要么是黑色；根节点是黑色；每个叶子节点（NIL 节点，空节点）是黑色；如果一个节点是红色的，则它的两个子节点都是黑色的；对每个节点，从该节点到其所有后代叶节点的简单路径上，均包含相同数目的黑色节点）来维持大致的平衡。红黑树的高度通常比 AVL 树略高，其高度上限为 2log(n + 1)，因此查询操作的时间复杂度同样为 (log n)，但在实际应用中，由于树的高度相对较高，其查询性能可能会略逊于 AVL 树。

在查询性能上，AVL 树由于其严格的平衡特性，表现会稍好于红黑树，但差距通常不大。

2、插入性能的对比：

- **AVL 树**：在插入新节点后，AVL 树可能会破坏原有的平衡结构，需要通过旋转操作（单旋转或双旋转）来重新平衡树。由于 AVL 树对平衡的要求非常严格，插入操作后可能需要进行多次旋转来恢复平衡，特别是在树的高度较高时，插入操作可能会引发较多的旋转操作，导致插入性能受到一定影响。插入操作的平均时间复杂度虽然也是 O(log n)，但由于旋转操作的开销，实际插入效率相对较低。
- **红黑树**：红黑树在插入新节点后，同样可能会破坏树的平衡，但它只需要进行少量的颜色调整和最多两次旋转操作就能恢复平衡。红黑树的平衡规则相对宽松，使得在插入操作时不需要像 AVL 树那样频繁地进行旋转操作，因此插入性能相对较好。插入操作的平均时间复杂度同样为 O(log n)，但由于减少了旋转操作的次数，实际插入效率更高。

在插入性能上，红黑树由于其弱平衡特性，表现优于 AVL 树。

在实际应用中，如果查询操作频繁，对查询性能要求较高，且插入和删除操作相对较少，可以选择 AVL 树；如果插入和删除操作较为频繁，对插入性能有较高要求，同时查询性能也能接受一定的损耗，则红黑树是更好的选择。例如，Java 中的 `TreeMap` 和 `TreeSet` 底层使用的就是红黑树，以兼顾插入、删除和查询操作的性能。

## 二叉树搜索最坏的时间复杂度，为什么会这样？以及用什么结果解决？

**当每次插入的元素都是二叉查找树中最大的元素，二叉查找树就会退化成了一条链表，查找数据的时间复杂度变成了 O(n)**，如下动图演示：

![img](./图片/1719298326342-05359c13-7dc9-473d-9b7c-035bc044c2c9.gif)

二叉查找树由于存在退化成链表的可能性，会使得查询操作的时间复杂度从 O(logn) 升为 O(n)。

为了解决二叉查找树会在极端情况下退化成链表的问题，后面就有人提出**平衡二叉查找树（AVL 树）**。

主要是在二叉查找树的基础上增加了一些条件约束：**每个节点的左子树和右子树的高度差不能超过 1**。也就是说节点的左子树和右子树仍然为平衡二叉树，这样查询操作的时间复杂度就会一直维持在 O(logn) 。

下图是每次插入的元素都是平衡二叉查找树中最大的元素，可以看到，它会维持自平衡：

![img](./图片/1719298358056-6b32c38a-32df-429c-a4c5-6c3e07cbd360.gif)

除了平衡二叉查找树，还有很多自平衡的二叉树，比如红黑树，它也是通过一些约束条件来达到自平衡，不过红黑树的约束条件比较复杂。下面是红黑树插入节点的过程，这左旋右旋的操作，就是为了自平衡。

![img](./图片/1719298358403-d3be5886-1cd9-4b07-888d-4476e42f8fcc.gif)

## B+树的特点是什么？

- B+树是一种自平衡的多路查找树，所有叶节点都位于同一层，保证了树的平衡，使得搜索、插入和删除操作的时间复杂度为对数级别的。
- 非叶节点仅包含索引信息，不存储具体的数据记录，它们只用来引导搜索到正确的叶节点。非叶节点的子树指针与关键字数量相同，每个子树指针指向一个子树，子树中的所有键值都在某个区间内。
- 所有数据记录都存储在叶节点中，且叶节点中的数据是按关键字排序的。叶节点包含实际的数据和关键字，它们是数据存储和检索的实体单元。叶节点之间通过指针相互链接，形成一个链表，便于范围查询和顺序遍历。

### B+树和B树有什么不一样，B+树的叶子节点和非叶子节点有什么不一样，非叶子节点会不会存数据？

- **检索路径**：B树在查找数据时，可能在非叶子节点找到目标数据，路径长度不固定。即查找时可以在任意一个节点终止。B+树中所有数据都在叶子节点，查找数据时必须走到叶子节点，路径长度固定（均等）。即查找总是要到叶子节点结束。
- **叶子节点结构**：B树中叶子节点之间没有特别的链接，彼此独立。B+树中叶子节点通过指针连接，形成一个有序链表，便于范围查询和顺序访问。
- **非叶子节点内容**：B树中非叶子节点存储数据和索引。B+树中非叶子节点只存储索引，不存储实际数据。因此，当数据量比较大时，相对于B树，B+树的层高更少，查找效率也就更高。
- **高效地范围查询：**B+树叶子节点采用的是双链表连接，适合 MySQL 中常见的基于范围的顺序查找，而 B 树在进行范围查询时需要进行中序遍历，性能较差。

### 红黑树，b树，b+树有什么区别？

| **特性**         | **红黑树**          | **B树**              | **B+树**                       |
| ---------------- | ------------------- | -------------------- | ------------------------------ |
| **节点容量**     | 每个节点存1个键值对 | 每个节点存多个键值对 | 非叶节点存多个键，仅叶节点存值 |
| **分支数**       | 二分叉（二叉树）    | 多分叉（多路树）     | 多分叉，叶节点额外链表连接     |
| **数据存储位置** | 所有节点均存数据    | 所有节点均存数据     | 数据仅存于叶节点               |
| **平衡维护方式** | 颜色标记与旋转      | 节点分裂/合并        | 节点分裂/合并                  |
| **查找稳定性**   | 查到即返回          | 查到即返回           | 必须查至叶节点                 |

核心区别：

- **查询效率：红黑树**单点查询稳定在 `O(log n)`，但树深度较高（如1亿数据需约30次查找）。**B/B+树**：树高更低（相同数据量下树高度可能仅为红黑树的1/3），显著减少磁盘I/O次数。
- **范围查询：B+树**的叶子节点通过指针形成链表，范围遍历效率极高（如查询`[A, Z]`只需找到起点后顺序遍历）。**B树/红黑树**：范围查询需回溯父节点或频繁调整指针，效率较低。
- **插入/删除维护成本：红黑树**需频繁旋转和变色，维护规则复杂。**B/B+树**：通过批量调整（分裂/合并节点）减少频繁操作，更适合海量数据。
- **场景选择**：如果数据在内存中 + 高频增删 ，选择红黑树更合适，如果数据在磁盘 + 随机/范围查询均需 ，选择B+树更合适。

## 堆是什么？

堆是一颗**完全二叉树**，这样实现的堆也被称为**二叉堆**。堆中节点的值都大于等于（或小于等于）其子节点的值，堆中如果节点的值都大于等于其子节点的值，我们把它称为**大顶堆**，如果都小于等于其子节点的值，我们将其称为**小顶堆**。





下图中，1，2 是大顶堆，3 是小顶堆， 4 不是堆（不是完全二叉树）

![image-20240725233642091](./图片/image-20240725233642091.png)

## 前缀树是什么？有什么应用？

前缀树（Trie Tree），也称为字典树、单词查找树或键树，是一种树形数据结构。它的核心思想是利用字符串的公共前缀来减少查询时间，最大限度地减少无谓的字符串比较，从而提高查询效率。

前缀树的特点是：

- **根节点不包含字符**：除根节点外的每一个节点都包含一个字符。
- **从根节点到某一节点**：路径上经过的字符连接起来，即为该节点对应的字符串。
- **每个节点的所有子节点包含的字符都不相同**。

设我们有字符串集合 `{"apple", "apricot", "banana", "cherry"}`，构建的前缀树结构大致如下：

```java
        root
       / | \
      a  b  c
     /    |    \
    p     a     h
   /     /       \
  p     n         e
 /     /           \
l     a             r
 \   /               \
  e t                 r
                     /
                    r
```

前缀树的应用场景：

- **字符串检索：**比如在搜索引擎的搜索框中，当用户输入部分关键词时，搜索引擎可以利用前缀树快速找到以该关键词为前缀的所有相关词条，实现自动补全功能。又比如将所有正确的单词存储在前缀树中，当用户输入一个单词时，通过在前缀树中查找该单词是否存在，来判断其拼写是否正确。
- **路由表匹配**：在网络路由中，路由器需要根据 IP 地址的前缀来选择合适的路由。前缀树可以高效地实现 IP 地址的最长前缀匹配，从而快速确定数据包的转发路径。
- **词频统计：**可以将文本中的所有单词插入到前缀树中，同时在节点中记录每个单词的出现次数。通过遍历前缀树，可以统计出文本中每个单词的词频。

## LRU是什么？如何实现？

LRU 是一种缓存淘汰算法，当缓存空间已满时，优先淘汰最长时间未被访问的数据。

实现的方式是哈希表+双向链表结合。

![image-20240725233704781](./图片/image-20240725233704781.png)

具体实现步骤如下：

- 使用哈希表存储数据的键值对，键为缓存的键，值为对应的节点。
- 使用双向链表存储数据节点，链表头部为最近访问的节点，链表尾部为最久未访问的节点。
- 当数据被访问时，如果数据存在于缓存中，则将对应节点移动到链表头部；如果数据不存在于缓存中，则将数据添加到缓存中，同时创建一个新节点并插入到链表头部。
- 当缓存空间已满时，需要淘汰最久未访问的节点，即链表尾部的节点。

上面这种思想方式，LRU 算法可以在 O(1) 的时间复杂度内实现数据的插入、查找和删除操作。每次访问数据时，都会将对应的节点移动到链表头部，保证链表头部的节点是最近访问的数据，而链表尾部的节点是最久未访问的数据。当缓存空间不足时，淘汰链表尾部的节点即可。

## **布隆过滤器怎么设计？时间复杂度？**

在开发过程中，经常要判断一个元素是否在一个集合中。假设你现在要给项目添加IP黑名单功能，此时你手上有大约 1亿个恶意IP的数据集，有一个IP发起请求，你如何判断这个IP在不在你的黑名单中？

类似这种问题用Java自己的Collection和Map很难处理，因为它们存储元素本身，会造成内存不足，而我们只关心元素存不存在，对于元素的值我们并不关心，具体值是什么并不重要。

「**布隆过滤器**」可以用来解决类似的问题，具有运行快速，内存占用小的特点，它是一个保存了很长的二级制向量，同时结合 Hash 函数实现的。而高效插入和查询的代价就是，它是一个基于概率的数据结构，**只能告诉我们一个元素绝对不在集合内，对于存在集合内的元素有一定的误判率**。

布隆过滤器中总是会存在误判率，因为哈希碰撞是不可能百分百避免的。布隆过滤器对这种误判率称之为「**假阳性概率**」，即：**False Positive Probability**，简称为 fpp。在实践中使用布隆过滤器时可以自己定义一个 fpp，然后就可以根据布隆过滤器的理论计算出需要多少个哈希函数和多大的位数组空间。需要注意的是这个 fpp 不能定义为 100%，因为无法百分保证不发生哈希碰撞。

下图表示向布隆过滤器中添加元素 `www.123.com` 和 `www.456.com` 的过程，它使用了 func1 和 func2 两个简单的哈希函数。

![image-20240820113041030](./图片/image-20240820113041030.png)

其基本原理如下：

1. **初始化**：当我们创建一个布隆过滤器时，我们首先创建一个全由0组成的位数组（bit array)。同时，我们还需选择几个独立的哈希函数，每个函数都可以将集合中的元素映射到这个位数组的某个位置。
2. **添加元素**：在布隆过滤器中添加一个元素时，我们会将此元素通过所有的哈希函数进行映射，得到在位数组中的几个位置，然后将这些位置标记为1。
3. **查询元素**：如果我们要检查一个元素是否在集合中，我们同样使用这些哈希函数将元素映射到位数组中的几个位置，**如果所有的位置都被标记为1，那么我们就可以说该元素可能在集合中。如果有任何一个位置不为1，那么该元素肯定不在集合中**。

通过其原理可以知道，我们可以提高数组长度以及 hash 计算次数来降低误报率，但是相应的 CPU、内存的消耗也会相应地提高，会增加存储和计算的开销。因此，布隆过滤器的使用需要在误判率和性能之间进行权衡。布隆过滤器有以下两个特点：

- **只要返回数据不存在，则肯定不存在。**
- **返回数据存在，不一定存在**。

布隆过滤器的误判率主要来源于「**哈希碰撞**」。因为位数组的大小有限，不同的元素可能会被哈希到相同的位置，导致即使某个元素并未真正被加入过滤器，也可能因为其他已经存在的元素而让所有哈希函数映射的位都变为了1，从而误判为存在。这就是布隆过滤器的“假阳性”错误。在有限的数组长度中存放大量的数据，即便是再完美的 Hash 算法也会有冲突，所以有可能两个完全不同的 A、B 两个数据最后定位到的位置是一模一样的。这时拿 B 进行查询时那自然就是误报了。

**布隆过滤器的时间复杂度和空间复杂度：**对于一个 m（比特位个数）和 k（哈希函数个数）值确定的布隆过滤器，添加和判断操作的时间复杂度都是 O(k)，这意味着每次你想要插入一个元素或者查询一个元素是否在集合中，只需要使用 k 个哈希函数对该元素求值，然后将对应的比特位标记或者检查对应的比特位即可。

# 简答算法

## 排序算法

### 说几个你懂的排序算法，并说明其时间空间复杂度

![image-20240725233717608](./图片/image-20240725233717608.png)

- **冒泡排序**：通过相邻元素的比较和交换，每次将最大（或最小）的元素逐步“冒泡”到最后（或最前）。时间复杂度：最好情况下O(n)，最坏情况下O(n^2)，平均情况下O(n^2)。，空间复杂度：O(1)。
- **插入排序**：将待排序元素逐个插入到已排序序列的合适位置，形成有序序列。时间复杂度：最好情况下O(n)，最坏情况下O(n^2)，平均情况下O(n^2)，空间复杂度：O(1)。
- **选择排序**（Selection Sort）：通过不断选择未排序部分的最小（或最大）元素，并将其放置在已排序部分的末尾（或开头）。时间复杂度：最好情况下O(n^2)，最坏情况下O(n^2)，平均情况下O(n^2)，空间复杂度：O(1)。
- **快速排序**（Quick Sort）：通过选择一个基准元素，将数组划分为两个子数组，使得左子数组的元素都小于（或等于）基准元素，右子数组的元素都大于（或等于）基准元素，然后对子数组进行递归排序。时间复杂度：最好情况下O(nlogn)，最坏情况下O(n^2)，平均情况下O(nlogn)，空间复杂度：最好情况下O(logn)，最坏情况下O(n)。
- **归并排序**（Merge Sort）：将数组不断分割为更小的子数组，然后将子数组进行合并，合并过程中进行排序。时间复杂度：最好情况下O(nlogn)，最坏情况下O(nlogn)，平均情况下O(nlogn)。空间复杂度：O(n)。
- **堆排序**（Heap Sort）：通过将待排序元素构建成一个最大堆（或最小堆），然后将堆顶元素与末尾元素交*换，再重新调整堆，重复该过程直到排序完成。时间复杂度：最好情况下O(nlogn)，最坏情况下O(nlogn)，平均情况下O(nlogn)。空间复杂度：O(1)。

### 讲一下冒泡排序算法

冒泡排序：通过相邻元素的比较和交换，每次将最大（或最小）的元素逐步“冒泡”到最后（或最前）

- 冒泡排序的最好时间复杂度出现在以下情况：当待排序数组已经有序时，即每个元素都比其前面的元素小，那么在第一次遍历数组时就可以确定排序已经完成，因此时间复杂度为O(n)。
- 冒泡排序的时间复杂度为O(n^2)。因为在排序过程中，需要进行多次遍历和元素交换，而每次遍历都需要比较相邻的元素并决定是否进行交换，这种操作需要花费O(n)的时间。因此，冒泡排序的时间复杂度通常为O(n^2)。

```go
// 冒泡排序算法
func bubbleSort(arr []int) {
    n := len(arr)
    
    // 外层循环控制比较轮数
    for i := 0; i < n-1; i++ {
        // 内层循环进行两两比较并交换
        for j := 0; j < n-i-1; j++ {
            if arr[j] > arr[j+1] {
                // 交换两个元素
                arr[j], arr[j+1] = arr[j+1], arr[j]
            }
        }
    }
}

func main() {
    arr := []int{64, 34, 25, 12, 22, 11, 90}
    bubbleSort(arr)
    fmt.Println("Sorted array:")
    for _, value := range arr {
        fmt.Print(value, " ")
    }
}
```

### 讲一下快排原理

快排使用了分治策略的思想，所谓分治，顾名思义，就是分而治之，将一个复杂的问题，分成两个或多个相似的子问题，在把子问题分成更小的子问题，直到更小的子问题可以简单求解，求解子问题，则原问题的解则为子问题解的合并。

快排的过程简单的说只有三步：

- 首先从序列中选取一个数作为基准数
- 将比这个数大的数全部放到它的右边，把小于或者等于它的数全部放到它的左边 （一次快排 `partition`）
- 然后分别对基准的左右两边重复以上的操作，直到数组完全排序

具体按以下步骤实现：

- 1，创建两个指针分别指向数组的最左端以及最右端
- 2，在数组中任意取出一个元素作为基准
- 3，左指针开始向右移动，遇到比基准大的停止
- 4，右指针开始向左移动，遇到比基准小的元素停止，交换左右指针所指向的元素
- 5，重复3，4，直到左指针超过右指针，此时，比基准小的值就都会放在基准的左边，比基准大的值会出现在基准的右边
- 6，然后分别对基准的左右两边重复以上的操作，直到数组完全排序

注意这里的基准该如何选择？最简单的一种做法是每次都是选择最左边的元素作为基准，但这对几乎已经有序的序列来说，并不是最好的选择，它将会导致算法的最坏表现。还有一种做法，就是选择中间的数或通过 `Math.random()` 来随机选取一个数作为基准。

![图片](./图片/640.gif)

代码实现：

```go
// 快速排序算法
func quickSort(arr []int, low, high int) {
    if low < high {
        pi := partition(arr, low, high)

        // 递归排序左半部分
        quickSort(arr, low, pi-1)
        // 递归排序右半部分
        quickSort(arr, pi+1, high)
    }
}

// 划分函数，用于找到基准元素的正确位置
func partition(arr []int, low, high int) int {
    pivot := arr[high] // 选择最后一个元素作为基准
    i := low - 1       // 初始化较小元素的索引

    for j := low; j < high; j++ {
        if arr[j] < pivot {
            i++
            // 交换元素
            arr[i], arr[j] = arr[j], arr[i]
        }
    }

    // 将基准元素放到正确的位置
    arr[i+1], arr[high] = arr[high], arr[i+1]

    return i + 1 // 返回基准元素的位置
}

func main() {
    arr := []int{10, 7, 8, 9, 1, 5}
    quickSort(arr, 0, len(arr)-1)
    fmt.Println("Sorted array:")
    for _, value := range arr {
        fmt.Print(value, " ")
    }
}
```

### 堆排序算法原理，稳定吗？

如果每个节点大于等于子树中的每个节点，我们称之为大顶堆，小于等于子树中的每个节点，我们则称之为小顶堆。

![img](./图片/1719984315347-b887765d-bf9d-4781-9951-a9fc1e10490c.png)

堆的要求：

- 必须是完全二叉树
- 堆中的每一个节点，都必须大于等于（或小于等于）其子树中每个节点的值。

堆通常是使用**一维数组**进行保存，节省空间，不需要存左右子节点的指针，通过下标就可定位左右节点和父节点。在起始位置为0的数组中：

- 父节点 i 的左子节点在(2i+1)的位置
- 父节点 i 的右子节点在(2i+2)的位置
- 子节点 i 的父节点在(i-1)/2向下取整的位置

![img](./图片/1719985997397-ca423f41-4362-4a18-8611-91b8112d096c.png)

我们可以把堆排序的过程大致分为两大步骤，分别是建堆和排序。

- 建堆：建堆操作就是将一个无序的数组转化为最大堆的操作，首先将数组原地建一个堆。“原地”的含义就是不借助另一个数组，就在原数组上操作。我们的实现思路是从后往前处理数据，并且每个数据都是从上向下调整。
- 排序：建堆结束后，数组中的数据已经按照大顶堆的特性进行组织了，数组中的第一个元素就是堆顶，也就是最大的元素。我们把它和最后一个元素交换，那最大的元素就放到了下标为n的位置，时末尾元素就是最大值，将剩余元素重新堆化成一个大顶堆。继续重复这些步骤，直至数组有序排列

假设我们有一个数组 [4, 10, 3, 5, 1]，堆排序的过程如下：

1. 构建最大堆：[10, 5, 3, 4, 1]
2. 交换堆顶元素与最后一个元素：[1, 5, 3, 4, 10]
3. 调整剩余元素为堆：[5, 4, 3, 1]
4. 再次交换堆顶元素与最后一个元素：[1, 4, 3, 5]
5. 调整剩余元素为堆：[4, 3, 1]
6. 继续上述过程直到排序完成：[1, 3, 4, 5, 10]

算法实现：

```c
// 堆排序方法
func heapSort(arr []int) {
    n := len(arr)

    // 构建堆（重新排列数组）
    for i := n/2 - 1; i >= 0; i-- {
        heapify(arr, n, i)
    }

    // 依次从堆中提取元素
    for i := n - 1; i > 0; i-- {
        // 将当前根节点移动到末尾
        arr[0], arr[i] = arr[i], arr[0]

        // 在堆中调整
        heapify(arr, i, 0)
    }
}

// 通过索引i对数组arr的前n个元素进行堆调整
func heapify(arr []int, n, i int) {
    largest := i     // 初始化最大值索引
    left := 2*i + 1  // 左孩子节点
    right := 2*i + 2 // 右孩子节点

    // 如果左孩子大于根节点
    if left < n && arr[left] > arr[largest] {
        largest = left
    }

    // 如果右孩子大于当前最大值
    if right < n && arr[right] > arr[largest] {
        largest = right
    }

    // 如果最大值不是根节点
    if largest != i {
        // 交换元素
        arr[i], arr[largest] = arr[largest], arr[i]

        // 递归调整受影响的子树
        heapify(arr, n, largest)
    }
}

func main() {
    arr := []int{4, 10, 3, 5, 1}
    heapSort(arr)
    
    fmt.Println("排序后的数组:")
    for _, i := range arr {
        fmt.Print(i, " ")
    }
}
```

现在我们来分析一下堆排序的时间复杂度、空间复杂度以及稳定性。

- 整个堆排序的过程中，只需要个别的临时存储空间，所以**堆排序是原地排序算法**。
- 堆排序包括建堆和排序两个操作，建堆的时间复杂度是O(n)，排序过程时间复杂度是O(nlogN)。所以，**堆排序的整个时间复杂度是O(nlogN)**。
- 因为在排序的过程中，存在将堆的最后一个节点跟堆顶互换的操作，所以有可能会改变值相同数据的原始相对顺序，所以**堆排序不是稳定的排序算法**。例如，假设我们有两个相同的元素A和B，且A在B前面。在构建和调整堆的过程中，B可能被移动到A的前面，从而破坏了它们原来的相对顺序。

### **归并排序和快速排序的使用场景**

- 归并排序是稳定排序算法，适合排序稳定的场景；
- 快速排序是不稳定排序算法，不适合排序稳定的场景，快速排序是目前基于比较的内部排序中被认为是最好的方法，当待排序的关键字是随机分布时，快速排序的平均时间最短；

### 什么是排序稳定性？

排序算法的稳定性是指在排序过程中，当有多个具有相同关键字的元素时，这些元素在排序后的序列中保持它们原有的相对顺序。

换句话说，如果两个元素有相同的键值，那么在排序前，如果第一个元素在第二个元素之前，排序后第一个元素也应该在第二个元素之前。

具体来说，对于一个序列中的两个元素A和B，如果A和B的键值相同，且在排序前A在B之前，那么在排序后A仍然应该在B之前，算法才能被称为是稳定的。

例如，考虑一个包含姓名和年龄的列表：

```plain
[("Alice", 25), ("Bob", 25), ("Charlie", 20)]
```

如果排序算法是稳定的，那么在按年龄排序后，"Alice"和"Bob"的相对顺序不会改变：

```plain
[("Charlie", 20), ("Alice", 25), ("Bob", 25)]
```

但如果排序算法不稳定，"Alice"和"Bob"的相对顺序可能会在排序后改变：

```plain
[("Charlie", 20), ("Bob", 25), ("Alice", 25)]
```

在这种情况下，排序算法就被认为是不稳定的

### **稳定和不稳定排序算法有什么特点？**

稳定排序算法的特点：

- 相同元素的相对位置不会改变，排序后仍然保持原始顺序。
- 适用于需要保持元素间相对顺序关系的场景，如按照年龄排序后按姓名排序。

不稳定排序算法的特点：

- 相同元素的相对位置可能会改变，排序后不保证原始顺序。
- 可能会更快，但不适用于需要保持元素间相对顺序关系的场景

### 说说快排流程，时间复杂度

快速排序的流程如下：

- 从数组中选择一个基准元素（通常是数组中间位置的元素）。
- 将数组分成两部分，小于基准元素的放在左边，大于基准元素的放在右边。
- 递归地对左右两部分进行快速排序。

快速排序的时间复杂度为O(n log n)，其中n为数组的长度。最坏情况下时间复杂度为O(n^2)，发生在每次选择的基准元素都是最大或最小值时。平均情况下时间复杂度为O(n log n)，效率较高。

### 快排为什么时间复杂度最差是O（n^2）

主要是因为在每次划分时选择的基准元素不合适导致的。当每次选择的基准元素都是当前子数组中的最大或最小元素时，就会导致每次划分只能减少一个元素，而不是均匀地分成两部分，从而造成时间复杂度达到O(n^2)。

这种情况通常发生在数组已经有序或基本有序的情况下。为了避免最坏情况发生，可以通过随机选择基准元素或者使用三数取中法等策略来提高快速排序的性能。

### 快排这么强，那冒泡排序还有必要吗？

冒泡排序在一些特定场景下仍然有其优势，比如：

- 对于小规模数据或基本有序的数据，冒泡排序可能比快速排序更简单、更直观。
- 冒泡排序是稳定排序算法，相对于快速排序的不稳定性，在某些情况下可能更适合要求稳定性的场景。
- 冒泡排序是原地排序算法，不需要额外的空间，适合空间复杂度要求严格的场景。

### 如果要对一个很大的数据集，进行排序，而没办法一次性在内存排序，这时候怎么办？

可以使用外部排序来解决，基本思路分为两个阶段。

> 部分排序阶段。

我们根据内存大小，将待排序的文件拆成多个部分，使得每个部分都是足以存入内存中的。然后选择合适的内排序算法，将多个文件部分排序，并输出到容量可以更大的外存临时文件中，每个临时文件都是有序排列的，我们将其称之为一个“顺段”。

在第一个阶段部分排序中，由于内存可以装下每个顺段的所有元素，可以使用**快速排序**，时间复杂度是O(nlogn)。

> 归并阶段

我们对前面的多个“顺段”进行合并，思想和归并排序其实是一样的。以 2 路归并为例，每次都将两个连续的顺段合并成一个更大的顺段。

因为内存限制，每次可能只能读入两个顺段的部分内容，所以我们需要一部分一部分读入，在内存里将可以确定顺序的部分排列，并输出到外存里的文件中，不断重复这个过程，直至两个顺段被完整遍历。这样经过多层的归并之后，最终会得到一个完整的顺序文件。

![image-20240725233745307](./图片/image-20240725233745307.png)

归并阶段有个非常大的时间消耗就是 IO，也就是输入输出。最好就是让归并的层数越低越好，为了降低降低归并层数，可以使用**败者树**。

败者树中的非终端结点中存储的是胜利（左右孩子相比较，谁最小即为胜者）的一方；而败者树中的非终端结点存储的是失败的一方。而在比较过程中，都是拿胜者去比较。

![image-20240725233754449](./图片/image-20240725233754449.png)

现在有了败者树的加持，多路归并排序就可以比较高效地解决外部排序的问题了。

大致思路就是：

- 先用内排序算法（比如快速排序），尽可能多的加载源文件，将其变成 n 个有序顺段。
- 在内存有限的前提下每 k 个文件为一组，每次流式地从各个文件中读取一个单词，借助败者树选出字典序最低的一个，输出到文件中，这样就可以将 k 个顺段合并到一个顺段中了；反复执行这样的操作，直至所有顺段被归并到同一个顺段。

### 数组长度为N，找出最大的前K个值，怎么设计这个算法？时间复杂度是多少

取常见的解决方案有几种：

1. **排序法**：对整个数组进行排序，然后取前K个元素。比如使用快速排序，时间复杂度是O(N log N)。不过如果K远小于N，这种方法可能效率不高，因为排序整个数组没有必要。代码实现如下：

```java
import java.util.Arrays;

public class TopKBySorting {
    public static int[] topK(int[] arr, int k) {
        Arrays.sort(arr);
        int n = arr.length;
        int[] result = new int[k];
        for (int i = 0; i < k; i++) {
            result[i] = arr[n - k + i];
        }
        return result;
    }

    public static void main(String[] args) {
        int[] arr = {3, 1, 4, 1, 5, 9, 2, 6, 5, 3, 5};
        int k = 3;
        int[] topK = topK(arr, k);
        for (int num : topK) {
            System.out.print(num + " ");
        }
    }
}
```

1. **堆（优先队列）**：使用一个最小堆，维护K个最大的元素。遍历数组，当堆的大小小于K时直接加入，否则比较当前元素和堆顶，如果更大就替换堆顶，并调整堆。这样时间复杂度是O(N log K)，因为每次堆操作是O(log K)，需要进行N次。这种方法适合处理大数据流的情况，因为不需要一次性加载所有数据到内存。代码实现如下：

```java
import java.util.PriorityQueue;

public class TopKByMinHeap {
    public static int[] topK(int[] arr, int k) {
        PriorityQueue<Integer> minHeap = new PriorityQueue<>(k);
        for (int num : arr) {
            if (minHeap.size() < k) {
                minHeap.offer(num);
            } else if (num > minHeap.peek()) {
                minHeap.poll();
                minHeap.offer(num);
            }
        }
        int[] result = new int[k];
        for (int i = k - 1; i >= 0; i--) {
            result[i] = minHeap.poll();
        }
        return result;
    }

    public static void main(String[] args) {
        int[] arr = {3, 1, 4, 1, 5, 9, 2, 6, 5, 3, 5};
        int k = 3;
        int[] topK = topK(arr, k);
        for (int num : topK) {
            System.out.print(num + " ");
        }
    }
}
```

1. **快速选择算法**：基于快速排序的partition思想，找到第K大的元素，然后取前K个。平均时间复杂度是O(N)，最坏情况下是O(N²)，但可以通过随机化选择pivot来优化，使得最坏情况概率很低。这种方法在数据可以全部放入内存时效率很高，尤其是当K比较大时。代码实现如下：

```java
import java.util.Arrays;

public class TopKByQuickSelect {
    public static int[] topK(int[] arr, int k) {
        quickSelect(arr, 0, arr.length - 1, k);
        int[] result = Arrays.copyOfRange(arr, arr.length - k, arr.length);
        Arrays.sort(result);
        return result;
    }

    private static void quickSelect(int[] arr, int left, int right, int k) {
        if (left < right) {
            int pivotIndex = partition(arr, left, right);
            if (pivotIndex > arr.length - k) {
                quickSelect(arr, left, pivotIndex - 1, k);
            } else if (pivotIndex < arr.length - k) {
                quickSelect(arr, pivotIndex + 1, right, k);
            }
        }
    }

    private static int partition(int[] arr, int left, int right) {
        int pivot = arr[right];
        int i = left - 1;
        for (int j = left; j < right; j++) {
            if (arr[j] < pivot) {
                i++;
                swap(arr, i, j);
            }
        }
        swap(arr, i + 1, right);
        return i + 1;
    }

    private static void swap(int[] arr, int i, int j) {
        int temp = arr[i];
        arr[i] = arr[j];
        arr[j] = temp;
    }

    public static void main(String[] args) {
        int[] arr = {3, 1, 4, 1, 5, 9, 2, 6, 5, 3, 5};
        int k = 3;
        int[] topK = topK(arr, k);
        for (int num : topK) {
            System.out.print(num + " ");
        }
    }
}
```

具体怎么选，可以根据具体需求选择合适的方法：

- 如果 *K* 很小，推荐使用 **最小堆** 方法，其时间复杂度为 O(NlogK)。
- 如果 *K* 较大或需要线性时间复杂度，推荐使用 **快速选择** 方法，其平均时间复杂度为 O(N)。
- 如果对实现复杂度要求较低且 *K* 接近 *N*，可以直接使用 **排序法** ，时间复杂度为 O(NlogN)。